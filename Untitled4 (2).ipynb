{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx1AOomEbgUc",
        "outputId": "7b191dd9-e3cb-41e3-a05b-fc0acf7442ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available()) # Ph·∫£i tr·∫£ v·ªÅ True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "sbeFNH2PhN_o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NguyenPhanNhatLan/medical_re.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ1gM3bCbiQV",
        "outputId": "7e9e34dd-6c9d-4ea3-9707-3fd2127fc3a2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'medical_re'...\n",
            "remote: Enumerating objects: 305, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 305 (delta 66), reused 44 (delta 34), pack-reused 208 (from 1)\u001b[K\n",
            "Receiving objects: 100% (305/305), 6.78 MiB | 3.98 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd medical_re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuJv9YLtbkEH",
        "outputId": "78bccabb-7019-4d3c-f354-3ecf3bf7dec3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/medical_re/medical_re\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup, AutoTokenizer\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from utils.collator import RBERTCollator, BERTESCollator\n",
        "from datasets.rbert_dataset import RBERTDataset\n",
        "from datasets.bert_es_dataset import BERTESDataset\n",
        "from encoder.vihealth_encoder import ViHealthBERTEncoder\n",
        "from models.r_bert import RBERT\n",
        "from models.bert_es import BERTES\n",
        "\n",
        "def load_config(path=\"config.yaml\"):\n",
        "    with open(path, 'r') as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def get_device():\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    elif torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    else:\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "def clean_memory():\n",
        "    \"\"\"D·ªçn s·∫°ch r√°c trong RAM ƒë·ªÉ tr√°nh crash notebook\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.backends.mps.is_available():\n",
        "        torch.mps.empty_cache()\n",
        "    elif torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    print(\"üßπ Memory cleaned!\")"
      ],
      "metadata": {
        "id": "jl1nlpcGbkzG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "# --- C·∫•u h√¨nh chung ---\n",
        "project_name: \"medical_re_optimization_notebook\"\n",
        "seed: 42\n",
        "output_dir: \"./outputs_tuning\"\n",
        "\n",
        "# --- Model & Data ---\n",
        "model_type: \"rbert\"         # \"rbert\" ho·∫∑c \"bertes\"\n",
        "encoder_type: \"vihealth\"    # \"vihealth\"\n",
        "num_labels: 5\n",
        "\n",
        "# --- T√†i nguy√™n & Training (C·ªë ƒë·ªãnh) ---\n",
        "fixed_params:\n",
        "  max_epochs: 5             # Search nhanh\n",
        "  patience: 2               # Early stopping s·ªõm\n",
        "  grad_clip_norm: 1.0\n",
        "  accumulation_steps: 4     # Quan tr·ªçng cho m√°y RAM y·∫øu (Batch th·ª±c = batch_size * 4)\n",
        "\n",
        "# --- Kh√¥ng gian t√¨m ki·∫øm (Grid Search) ---\n",
        "search_space:\n",
        "  learning_rate: [1.0e-5, 2.0e-5, 3.0e-5]\n",
        "  batch_size: [8]           # Gi·ªØ nguy√™n 8 ƒë·ªÉ an to√†n RAM\n",
        "  dropout_rate: [0.1, 0.15]\n",
        "  warmup_ratio: [0.1]\n",
        "  max_length: [256]         # Gi·∫£m xu·ªëng 128 n·∫øu v·∫´n b·ªã OOM\n",
        "  weight_decay: [0.01]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIA3tudubu20",
        "outputId": "187cdf2c-0981-49cb-c0bf-070976119dac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_components(config, hparams, tokenizer=None):\n",
        "    model_type = config['model_type']\n",
        "    encoder_type = config['encoder_type']\n",
        "\n",
        "    # 1. Ch·ªçn Encoder Class\n",
        "    if encoder_type == \"vihealth\":\n",
        "        EncoderClass = ViHealthBERTEncoder\n",
        "        model_name = \"demdecuong/vihealthbert-base-word\"\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Encoder type {encoder_type} ch∆∞a ƒë∆∞·ª£c h·ªó tr·ª£ trong notebook n√†y.\")\n",
        "\n",
        "    if model_type == \"rbert\":\n",
        "        ModelClass = RBERT\n",
        "        DatasetClass = RBERTDataset\n",
        "        CollatorClass = RBERTCollator\n",
        "    elif model_type == \"bertes\":\n",
        "        ModelClass = BERTES\n",
        "        DatasetClass = BERTESDataset\n",
        "        CollatorClass = BERTESCollator\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # 3. Load Tokenizer (N·∫øu ch∆∞a c√≥)\n",
        "    # L∆∞u √Ω: Tokenizer load 1 l·∫ßn d√πng chung ƒë·ªÉ ti·∫øt ki·ªám RAM & th·ªùi gian\n",
        "    if tokenizer is None:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        tokenizer.add_special_tokens({'additional_special_tokens': [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]})\n",
        "\n",
        "    train_dataset = DatasetClass(\n",
        "        json_path=\"data/1_train_model.json\",\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=hparams['max_length']\n",
        "    )\n",
        "    val_dataset = DatasetClass(\n",
        "        json_path=\"data/2_dev_model.json\",\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=hparams['max_length']\n",
        "    )\n",
        "    collator = CollatorClass(tokenizer)\n",
        "\n",
        "    encoder = EncoderClass(model_name=model_name)\n",
        "\n",
        "    if len(encoder.tokenizer) != len(tokenizer):\n",
        "        encoder.tokenizer = tokenizer\n",
        "        encoder.model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    model = ModelClass(\n",
        "        encoder=encoder,\n",
        "        hidden_size=encoder.hidden_size,\n",
        "        num_labels=config['num_labels'],\n",
        "        dropout_rate=hparams['dropout_rate']\n",
        "    )\n",
        "\n",
        "    return model, train_dataset, val_dataset, collator, tokenizer"
      ],
      "metadata": {
        "id": "9UZajzpebzNd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def evaluate_refined(model, dataloader, device, amp_enabled=True):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    labels_list = []\n",
        "    total_loss = 0\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # T·ª± ƒë·ªông d√πng FP16 khi eval\n",
        "            with autocast(enabled=amp_enabled):\n",
        "                if isinstance(model, RBERT):\n",
        "                    outputs = model(\n",
        "                        input_ids, attention_mask,\n",
        "                        batch['e1_mask'].to(device),\n",
        "                        batch['e2_mask'].to(device),\n",
        "                        labels\n",
        "                    )\n",
        "                    logits = outputs['logits']\n",
        "                    loss = outputs['loss']\n",
        "                else:\n",
        "                    logits = model(\n",
        "                        input_ids, attention_mask,\n",
        "                        batch['e1_pos'].to(device),\n",
        "                        batch['e2_pos'].to(device)\n",
        "                    )\n",
        "                    loss = loss_fct(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    # --- S·ª¨A METRIC ---\n",
        "    # T√≠nh Micro-F1 (Quan tr·ªçng cho RE).\n",
        "    # N·∫øu class 0 l√† \"Other/No Relation\", c√≥ th·ªÉ c√¢n nh·∫Øc labels=[1,2,3,4] ƒë·ªÉ lo·∫°i b·ªè n√≥ kh·ªèi metric\n",
        "    # ·ªû ƒë√¢y t√¥i ƒë·ªÉ average='micro' chung cho an to√†n tr∆∞·ªõc.\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels_list, preds, average='micro')\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return f1, avg_loss"
      ],
      "metadata": {
        "id": "Xz2zwpBsb052"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YYeLC4ceTXZ",
        "outputId": "92527293-ad0c-4804-d6a9-edfb19e9d74f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.7.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.18.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def objective_optimized(trial):\n",
        "    hparams = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True),\n",
        "        'batch_size': trial.suggest_categorical('batch_size', [16, 32]), # TƒÉng BS l√™n v√¨ d√πng FP16\n",
        "        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.3),\n",
        "        'warmup_ratio': trial.suggest_float('warmup_ratio', 0.05, 0.2),\n",
        "        'weight_decay': trial.suggest_float('weight_decay', 0.01, 0.1),\n",
        "        'max_length': 256\n",
        "    }\n",
        "\n",
        "    # Config\n",
        "    accum_steps = cfg['fixed_params']['accumulation_steps']\n",
        "    max_epochs = 3\n",
        "\n",
        "    # 2. Build Components (Load Tokenizer ngo√†i v√≤ng l·∫∑p n·∫øu ƒë∆∞·ª£c ƒë·ªÉ nhanh h∆°n)\n",
        "    # L∆∞u √Ω: D√πng ƒë∆∞·ªùng d·∫´n train_path v√† dev_path m·ªõi t·∫°o ·ªü B∆∞·ªõc 1\n",
        "    # S·ª≠a l·∫°i h√†m build_components ƒë·ªÉ nh·∫≠n path file data ƒë·ªông thay v√¨ hardcode\n",
        "    model, train_ds, val_ds, collator, tokenizer = build_components(cfg, hparams)\n",
        "    model.to(device)\n",
        "\n",
        "    # T·ªëi ∆∞u DataLoader\n",
        "    train_loader = DataLoader(train_ds, batch_size=hparams['batch_size'], shuffle=True,\n",
        "                              collate_fn=collator, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=hparams['batch_size'], shuffle=False,\n",
        "                            collate_fn=collator, num_workers=2, pin_memory=True)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=hparams['learning_rate'], weight_decay=hparams['weight_decay'])\n",
        "    scaler = GradScaler() # INIT SCALER CHO FP16\n",
        "\n",
        "    total_steps = len(train_loader) * max_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * hparams['warmup_ratio']), num_training_steps=total_steps)\n",
        "\n",
        "    best_f1_trial = 0.0\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # --- TRAINING V·ªöI MIXED PRECISION ---\n",
        "            with autocast():\n",
        "                if isinstance(model, RBERT):\n",
        "                    outputs = model(input_ids, attention_mask, batch['e1_mask'].to(device), batch['e2_mask'].to(device), labels)\n",
        "                    loss = outputs['loss']\n",
        "                else:\n",
        "                    logits = model(input_ids, attention_mask, batch['e1_pos'].to(device), batch['e2_pos'].to(device))\n",
        "                    loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "                loss = loss / accum_steps\n",
        "\n",
        "            # Scale loss v√† backward\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (step + 1) % accum_steps == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        # Evaluate tr√™n t·∫≠p DEV (kh√¥ng ph·∫£i Test)\n",
        "        val_f1, val_loss = evaluate_refined(model, val_loader, device, amp_enabled=True)\n",
        "\n",
        "        trial.report(val_f1, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        best_f1_trial = max(best_f1_trial, val_f1)\n",
        "\n",
        "    del model, optimizer, scaler\n",
        "    clean_memory()\n",
        "    return best_f1_trial"
      ],
      "metadata": {
        "id": "jGgxKgYSePr1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "clean_memory()\n",
        "cfg = load_config()\n",
        "device = get_device()\n",
        "\n",
        "# T·∫°o Study\n",
        "# direction=\"maximize\" v√¨ ta mu·ªën F1 c√†ng cao c√†ng t·ªët\n",
        "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "print(\"üöÄ STARTING OPTUNA STUDY...\")\n",
        "# n_trials=10: Ch·∫°y th·ª≠ 10 c·∫•u h√¨nh kh√°c nhau\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"‚úÖ OPTUNA FINISHED.\")\n",
        "print(f\"Best value (F1): {study.best_value}\")\n",
        "print(f\"Best params: {study.best_params}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# L∆∞u best params ƒë·ªÉ d√πng cho retrain\n",
        "best_hparams = study.best_params\n",
        "# Th√™m c√°c params c·ªë ƒë·ªãnh v√†o best_hparams ƒë·ªÉ code retrain ph√≠a d∆∞·ªõi ch·∫°y ƒë∆∞·ª£c\n",
        "best_hparams['max_length'] = 256\n",
        "best_hparams['batch_size'] = 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjC6XN5Rb2tX",
        "outputId": "5eec5147-c7c1-4c78-c344-a94677c5809f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 02:29:39,947] A new study created in memory with name: no-name-b18ba3c8-78df-45b7-ae71-be696e246976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Memory cleaned!\n",
            "üöÄ STARTING OPTUNA STUDY...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if best_hparams is None:\n",
        "    print(\"Kh√¥ng t√¨m th·∫•y c·∫•u h√¨nh n√†o ch·∫°y th√†nh c√¥ng. Vui l√≤ng ki·ªÉm tra l·∫°i.\")\n",
        "else:\n",
        "    print(f\"üöÄ Retraining FINAL MODEL with best params: {best_hparams}\")\n",
        "    clean_memory()\n",
        "\n",
        "    # TƒÉng max_epochs cho l·∫ßn train cu·ªëi (train k·ªπ h∆°n l√∫c search)\n",
        "    cfg['fixed_params']['max_epochs'] = 10\n",
        "\n",
        "    # 1. Build l·∫°i model t·ªët nh·∫•t\n",
        "    model, train_ds, val_ds, collator, tokenizer = build_components(cfg, best_hparams)\n",
        "    model.to(device)\n",
        "\n",
        "    # 2. Setup training components\n",
        "    train_loader = DataLoader(train_ds, batch_size=best_hparams['batch_size'], shuffle=True, collate_fn=collator, num_workers=0)\n",
        "    val_loader = DataLoader(val_ds, batch_size=best_hparams['batch_size'], shuffle=False, collate_fn=collator, num_workers=0)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=best_hparams['learning_rate'], weight_decay=best_hparams['weight_decay'])\n",
        "\n",
        "    total_steps = len(train_loader) * cfg['fixed_params']['max_epochs']\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(total_steps * best_hparams['warmup_ratio']),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    accum_steps = cfg['fixed_params']['accumulation_steps']\n",
        "    best_final_f1 = 0.0\n",
        "\n",
        "    # 3. Final Training Loop\n",
        "    for epoch in range(cfg['fixed_params']['max_epochs']):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Final Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "        for step, batch in enumerate(progress_bar):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            if isinstance(model, RBERT):\n",
        "                outputs = model(input_ids, attention_mask, batch['e1_mask'].to(device), batch['e2_mask'].to(device), labels)\n",
        "                loss = outputs['loss']\n",
        "            else:\n",
        "                logits = model(input_ids, attention_mask, batch['e1_pos'].to(device), batch['e2_pos'].to(device))\n",
        "                loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "            loss = loss / accum_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (step + 1) % accum_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['fixed_params']['grad_clip_norm'])\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                train_loss += loss.item() * accum_steps\n",
        "                progress_bar.set_postfix({'loss': train_loss / (step + 1)})\n",
        "\n",
        "        # Evaluate & Save Best Checkpoint\n",
        "        val_f1, val_loss = evaluate(model, val_loader, device)\n",
        "        print(f\"   Epoch {epoch+1}: F1={val_f1:.4f} | Loss={val_loss:.4f}\")\n",
        "\n",
        "        if val_f1 > best_final_f1:\n",
        "            best_final_f1 = val_f1\n",
        "            print(f\"   üíæ Saving new best model to {cfg['output_dir']}...\")\n",
        "\n",
        "            # Save tokenizer v√† encoder config\n",
        "            model.encoder.save_pretrained(cfg['output_dir'])\n",
        "\n",
        "            # Save to√†n b·ªô weights c·ªßa model (bao g·ªìm c·∫£ classifier head)\n",
        "            torch.save(model.state_dict(), os.path.join(cfg['output_dir'], \"best_model.pth\"))\n",
        "\n",
        "            # Save config t·ªët nh·∫•t ƒë·ªÉ d√πng l·∫°i\n",
        "            import json\n",
        "            with open(os.path.join(cfg['output_dir'], \"best_config.json\"), 'w') as f:\n",
        "                json.dump(best_hparams, f, indent=4)\n",
        "\n",
        "    print(f\"\\nDONE. Final Best F1: {best_final_f1:.4f}\")"
      ],
      "metadata": {
        "id": "Abep-GpKeetM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
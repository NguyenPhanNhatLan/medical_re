{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab902e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from utils.collator import RBERTCollator, BERTESCollator\n",
    "from datasets.rbert_dataset import RBERTDataset\n",
    "from datasets.bert_es_dataset import BERTESDataset\n",
    "from encoder.vihealth_encoder import ViHealthBERTEncoder\n",
    "from models.r_bert import RBERT\n",
    "from models.bert_es import BERTES\n",
    "\n",
    "def load_config(path=\"config.yaml\"):\n",
    "    with open(path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"D·ªçn s·∫°ch r√°c trong RAM ƒë·ªÉ tr√°nh crash notebook\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"üßπ Memory cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54ac85a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "# --- C·∫•u h√¨nh chung ---\n",
    "project_name: \"medical_re_optimization_notebook\"\n",
    "seed: 42\n",
    "output_dir: \"./outputs_tuning\"\n",
    "\n",
    "# --- Model & Data ---\n",
    "model_type: \"rbert\"         # \"rbert\" ho·∫∑c \"bertes\"\n",
    "encoder_type: \"vihealth\"    # \"vihealth\"\n",
    "num_labels: 5\n",
    "\n",
    "# --- T√†i nguy√™n & Training (C·ªë ƒë·ªãnh) ---\n",
    "fixed_params:\n",
    "  max_epochs: 5             # Search nhanh\n",
    "  patience: 2               # Early stopping s·ªõm\n",
    "  grad_clip_norm: 1.0\n",
    "  accumulation_steps: 4     # Quan tr·ªçng cho m√°y RAM y·∫øu (Batch th·ª±c = batch_size * 4)\n",
    "\n",
    "# --- Kh√¥ng gian t√¨m ki·∫øm (Grid Search) ---\n",
    "search_space:\n",
    "  learning_rate: [1.0e-5, 2.0e-5, 3.0e-5]\n",
    "  batch_size: [8]           # Gi·ªØ nguy√™n 8 ƒë·ªÉ an to√†n RAM\n",
    "  dropout_rate: [0.1, 0.2]\n",
    "  warmup_ratio: [0.1]\n",
    "  max_length: [256]         # Gi·∫£m xu·ªëng 128 n·∫øu v·∫´n b·ªã OOM\n",
    "  weight_decay: [0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_components(config, hparams, tokenizer=None):\n",
    "    model_type = config['model_type']\n",
    "    encoder_type = config['encoder_type']\n",
    "    \n",
    "    # 1. Ch·ªçn Encoder Class\n",
    "    if encoder_type == \"vihealth\":\n",
    "        EncoderClass = ViHealthBERTEncoder\n",
    "        model_name = \"demdecuong/vihealthbert-base-word\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Encoder type {encoder_type} ch∆∞a ƒë∆∞·ª£c h·ªó tr·ª£ trong notebook n√†y.\")\n",
    "\n",
    "    if model_type == \"rbert\":\n",
    "        ModelClass = RBERT\n",
    "        DatasetClass = RBERTDataset\n",
    "        CollatorClass = RBERTCollator\n",
    "    elif model_type == \"bertes\":\n",
    "        ModelClass = BERTES\n",
    "        DatasetClass = BERTESDataset\n",
    "        CollatorClass = BERTESCollator\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "    # 3. Load Tokenizer (N·∫øu ch∆∞a c√≥)\n",
    "    # L∆∞u √Ω: Tokenizer load 1 l·∫ßn d√πng chung ƒë·ªÉ ti·∫øt ki·ªám RAM & th·ªùi gian\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]})\n",
    "\n",
    "    train_dataset = DatasetClass(\n",
    "        json_path=\"data/train.json\", \n",
    "        tokenizer=tokenizer,\n",
    "        max_length=hparams['max_length']\n",
    "    )\n",
    "    val_dataset = DatasetClass(\n",
    "        json_path=\"data/dev.json\",   \n",
    "        tokenizer=tokenizer,\n",
    "        max_length=hparams['max_length']\n",
    "    )\n",
    "    collator = CollatorClass(tokenizer)\n",
    "\n",
    "    encoder = EncoderClass(model_name=model_name)\n",
    "    \n",
    "    if len(encoder.tokenizer) != len(tokenizer):\n",
    "        encoder.tokenizer = tokenizer \n",
    "        encoder.model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    model = ModelClass(\n",
    "        encoder=encoder,\n",
    "        hidden_size=encoder.hidden_size,\n",
    "        num_labels=config['num_labels'],\n",
    "        dropout_rate=hparams['dropout_rate']\n",
    "    )\n",
    "\n",
    "    return model, train_dataset, val_dataset, collator, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels_list = []\n",
    "    total_loss = 0\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            if isinstance(model, RBERT):\n",
    "                # RBERT nh·∫≠n masks cho entity\n",
    "                outputs = model(\n",
    "                    input_ids, attention_mask, \n",
    "                    batch['e1_mask'].to(device), \n",
    "                    batch['e2_mask'].to(device), \n",
    "                    labels\n",
    "                )\n",
    "                logits = outputs['logits']\n",
    "                loss = outputs['loss']\n",
    "            else:\n",
    "                # BERT-ES nh·∫≠n positions\n",
    "                logits = model(\n",
    "                    input_ids, attention_mask, \n",
    "                    batch['e1_pos'].to(device), \n",
    "                    batch['e2_pos'].to(device)\n",
    "                )\n",
    "                loss = loss_fct(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "    macro_f1 = f1_score(labels_list, preds, average='macro')\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return macro_f1, avg_loss\n",
    "\n",
    "def run_session(config, hparams, device, run_id):\n",
    "    print(f\"\\n>>> [Run {run_id}] Config: {hparams}\")\n",
    "    \n",
    "    # 1. Build Components\n",
    "    model, train_ds, val_ds, collator, tokenizer = build_components(config, hparams)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 2. Dataloader (num_workers=0 ƒë·ªÉ an to√†n nh·∫•t tr√™n Notebook/MPS)\n",
    "    train_loader = DataLoader(train_ds, batch_size=hparams['batch_size'], shuffle=True, collate_fn=collator, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=hparams['batch_size'], shuffle=False, collate_fn=collator, num_workers=0)\n",
    "    \n",
    "    # 3. Optimizer & Scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=hparams['learning_rate'], weight_decay=hparams['weight_decay'])\n",
    "\n",
    "    total_steps = len(train_loader) * config['fixed_params']['max_epochs']\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(total_steps * hparams['warmup_ratio']),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # 4. Training Loop\n",
    "    best_f1 = 0.0\n",
    "    accum_steps = config['fixed_params']['accumulation_steps']\n",
    "    max_epochs = config['fixed_params']['max_epochs']\n",
    "    patience = config['fixed_params']['patience']\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # D√πng tqdm nh∆∞ng t·∫Øt b·ªõt log chi ti·∫øt ƒë·ªÉ ƒë·ª° r·ªëi output grid search\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            if isinstance(model, RBERT):\n",
    "                outputs = model(input_ids, attention_mask, batch['e1_mask'].to(device), batch['e2_mask'].to(device), labels)\n",
    "                loss = outputs['loss']\n",
    "            else:\n",
    "                logits = model(input_ids, attention_mask, batch['e1_pos'].to(device), batch['e2_pos'].to(device))\n",
    "                loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "            # Gradient Accumulation Logic\n",
    "            loss = loss / accum_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % accum_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['fixed_params']['grad_clip_norm'])\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                train_loss += loss.item() * accum_steps\n",
    "        \n",
    "        # Evaluate cu·ªëi epoch\n",
    "        val_f1, val_loss = evaluate(model, val_loader, device)\n",
    "        print(f\"   Epoch {epoch+1}: F1={val_f1:.4f} | Loss={val_loss:.4f}\")\n",
    "        \n",
    "        # Early Stopping Check (ƒë∆°n gi·∫£n)\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"   Early stopping t·∫°i epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # --- QUAN TR·ªåNG: DELETE BI·∫æN ƒê·ªÇ GI·∫¢I PH√ìNG RAM CHO NOTEBOOK ---\n",
    "    # X√≥a tham chi·∫øu t·ªõi model v√† optimizer ƒë·ªÉ Garbage Collector ho·∫°t ƒë·ªông\n",
    "    del model, optimizer, scheduler, train_loader, val_loader, train_ds, val_ds\n",
    "    clean_memory()\n",
    "    \n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947218e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleaned!\n",
      "Using Device: mps\n",
      "Total configurations to test: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56afb30c246743f5a8a073d0b4e9a89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grid Search Progress:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [Run 1] Config: {'learning_rate': 1e-05, 'batch_size': 8, 'dropout_rate': 0.1, 'warmup_ratio': 0.1, 'max_length': 256, 'weight_decay': 0.01}\n",
      "‚ùå OOM Error at Run 1. Skipping config...\n",
      "üßπ Memory cleaned!\n",
      "\n",
      ">>> [Run 2] Config: {'learning_rate': 1e-05, 'batch_size': 8, 'dropout_rate': 0.2, 'warmup_ratio': 0.1, 'max_length': 256, 'weight_decay': 0.01}\n",
      "‚ùå OOM Error at Run 2. Skipping config...\n",
      "üßπ Memory cleaned!\n",
      "\n",
      ">>> [Run 3] Config: {'learning_rate': 2e-05, 'batch_size': 8, 'dropout_rate': 0.1, 'warmup_ratio': 0.1, 'max_length': 256, 'weight_decay': 0.01}\n",
      "‚ùå OOM Error at Run 3. Skipping config...\n",
      "üßπ Memory cleaned!\n",
      "\n",
      ">>> [Run 4] Config: {'learning_rate': 2e-05, 'batch_size': 8, 'dropout_rate': 0.2, 'warmup_ratio': 0.1, 'max_length': 256, 'weight_decay': 0.01}\n",
      "‚ùå OOM Error at Run 4. Skipping config...\n",
      "üßπ Memory cleaned!\n",
      "\n",
      ">>> [Run 5] Config: {'learning_rate': 3e-05, 'batch_size': 8, 'dropout_rate': 0.1, 'warmup_ratio': 0.1, 'max_length': 256, 'weight_decay': 0.01}\n",
      "‚ùå OOM Error at Run 5. Skipping config...\n",
      "üßπ Memory cleaned!\n",
      "\n",
      ">>> [Run 6] Config: {'learning_rate': 3e-05, 'batch_size': 8, 'dropout_rate': 0.2, 'warmup_ratio': 0.1, 'max_length': 256, 'weight_decay': 0.01}\n",
      "‚ùå OOM Error at Run 6. Skipping config...\n",
      "üßπ Memory cleaned!\n",
      "\n",
      "========================================\n",
      "‚úÖ SEARCH FINISHED.\n",
      "Best Macro F1: -1.0000\n",
      "Best Hparams: None\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup ban ƒë·∫ßu\n",
    "clean_memory() # D·ªçn d·∫πp tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu\n",
    "cfg = load_config()\n",
    "set_seed(cfg['seed'])\n",
    "device = get_device()\n",
    "print(f\"Using Device: {device}\")\n",
    "\n",
    "if not os.path.exists(cfg['output_dir']):\n",
    "    os.makedirs(cfg['output_dir'])\n",
    "\n",
    "# 2. T·∫°o kh√¥ng gian t√¨m ki·∫øm (Grid Search)\n",
    "keys, values = zip(*cfg['search_space'].items())\n",
    "search_space = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "print(f\"Total configurations to test: {len(search_space)}\")\n",
    "\n",
    "# 3. Ch·∫°y v√≤ng l·∫∑p Search\n",
    "results = []\n",
    "best_search_score = -1\n",
    "best_hparams = None\n",
    "\n",
    "# D√πng tqdm ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn ƒë·ªô t·ªïng\n",
    "for i, hparams in enumerate(tqdm(search_space, desc=\"Grid Search Progress\")):\n",
    "    try:\n",
    "        # G·ªçi session train cho 1 c·∫•u h√¨nh\n",
    "        score = run_session(cfg, hparams, device, run_id=i+1)\n",
    "        \n",
    "        # L∆∞u k·∫øt qu·∫£ t·ªët nh·∫•t\n",
    "        if score > best_search_score:\n",
    "            best_search_score = score\n",
    "            best_hparams = hparams\n",
    "            print(f\"NEW BEST FOUND: {score:.4f} with {hparams}\")\n",
    "            \n",
    "        results.append({\n",
    "            \"run_id\": i+1,\n",
    "            \"hparams\": hparams,\n",
    "            \"best_f1\": score\n",
    "        })\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"OOM Error at Run {i+1}. Skipping config...\")\n",
    "            clean_memory() #\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"‚úÖ SEARCH FINISHED.\")\n",
    "print(f\"Best Macro F1: {best_search_score:.4f}\")\n",
    "print(f\"Best Hparams: {best_hparams}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_hparams is None:\n",
    "    print(\"Kh√¥ng t√¨m th·∫•y c·∫•u h√¨nh n√†o ch·∫°y th√†nh c√¥ng. Vui l√≤ng ki·ªÉm tra l·∫°i.\")\n",
    "else:\n",
    "    print(f\"üöÄ Retraining FINAL MODEL with best params: {best_hparams}\")\n",
    "    clean_memory()\n",
    "    \n",
    "    # TƒÉng max_epochs cho l·∫ßn train cu·ªëi (train k·ªπ h∆°n l√∫c search)\n",
    "    cfg['fixed_params']['max_epochs'] = 10 \n",
    "    \n",
    "    # 1. Build l·∫°i model t·ªët nh·∫•t\n",
    "    model, train_ds, val_ds, collator, tokenizer = build_components(cfg, best_hparams)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 2. Setup training components\n",
    "    train_loader = DataLoader(train_ds, batch_size=best_hparams['batch_size'], shuffle=True, collate_fn=collator, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=best_hparams['batch_size'], shuffle=False, collate_fn=collator, num_workers=0)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=best_hparams['learning_rate'], weight_decay=best_hparams['weight_decay'])\n",
    "    \n",
    "    total_steps = len(train_loader) * cfg['fixed_params']['max_epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(total_steps * best_hparams['warmup_ratio']),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    accum_steps = cfg['fixed_params']['accumulation_steps']\n",
    "    best_final_f1 = 0.0\n",
    "    \n",
    "    # 3. Final Training Loop\n",
    "    for epoch in range(cfg['fixed_params']['max_epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Final Epoch {epoch+1}\", leave=False)\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            if isinstance(model, RBERT):\n",
    "                outputs = model(input_ids, attention_mask, batch['e1_mask'].to(device), batch['e2_mask'].to(device), labels)\n",
    "                loss = outputs['loss']\n",
    "            else:\n",
    "                logits = model(input_ids, attention_mask, batch['e1_pos'].to(device), batch['e2_pos'].to(device))\n",
    "                loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            \n",
    "            loss = loss / accum_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % accum_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['fixed_params']['grad_clip_norm'])\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                train_loss += loss.item() * accum_steps\n",
    "                progress_bar.set_postfix({'loss': train_loss / (step + 1)})\n",
    "        \n",
    "        # Evaluate & Save Best Checkpoint\n",
    "        val_f1, val_loss = evaluate(model, val_loader, device)\n",
    "        print(f\"   Epoch {epoch+1}: F1={val_f1:.4f} | Loss={val_loss:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_final_f1:\n",
    "            best_final_f1 = val_f1\n",
    "            print(f\"   üíæ Saving new best model to {cfg['output_dir']}...\")\n",
    "            \n",
    "            # Save tokenizer v√† encoder config\n",
    "            model.encoder.save_pretrained(cfg['output_dir'])\n",
    "            \n",
    "            # Save to√†n b·ªô weights c·ªßa model (bao g·ªìm c·∫£ classifier head)\n",
    "            torch.save(model.state_dict(), os.path.join(cfg['output_dir'], \"best_model.pth\"))\n",
    "            \n",
    "            # Save config t·ªët nh·∫•t ƒë·ªÉ d√πng l·∫°i\n",
    "            import json\n",
    "            with open(os.path.join(cfg['output_dir'], \"best_config.json\"), 'w') as f:\n",
    "                json.dump(best_hparams, f, indent=4)\n",
    "                \n",
    "    print(f\"\\n‚ú® DONE. Final Best F1: {best_final_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966754c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
